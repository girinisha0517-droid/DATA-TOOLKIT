{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA TOOLKIT"
      ],
      "metadata": {
        "id": "HXadF4Cb1GHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is NumPy, and why is it widely used in Python\n",
        "ANS. NumPy (Numerical Python) is an open-source Python library that provides support for large, multidimensional arrays and a vast collection of high-level mathematical functions to operate efficiently on this data.\n",
        "\n",
        "It is widely used in Python for several key reasons:\n",
        "\n",
        " . Performance and Memory Efficiency: NumPy arrays are significantly faster and more memory-efficient than standard Python lists for numerical operations because they store homogeneous (same-type) data in contiguous memory blocks and execute operations at optimized, compiled C speed.\n",
        "\n",
        " . Vectorization: It supports vectorized operations, which apply mathematical functions to entire arrays at once without requiring explicit Python loops, resulting in cleaner, more concise, and faster code.\n",
        "\n",
        " . Rich Mathematical Functionality: NumPy offers a comprehensive suite of mathematical, statistical, and linear algebra functions (e.g., matrix multiplication, Fourier transforms, random number generation) that are essential for scientific computing.\n",
        "\n",
        " . Foundation of the Scientific Ecosystem: NumPy arrays are the de facto standard for exchanging data in the Python scientific computing ecosystem; many other crucial libraries like Pandas, Matplotlib, SciPy, and scikit-learn are built upon NumPy and use its arrays as their primary data structures.\n"
      ],
      "metadata": {
        "id": "jXd1zeaP1MQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. How does broadcasting work in NumPy\n",
        "Ans.NumPy broadcasting allows arithmetic operations on arrays of different shapes by automatically adjusting the smaller array to match the larger array's shape for the purpose of the operation. This process is conceptual and efficient, avoiding actual data duplication in memory.\n",
        "\n",
        "Broadcasting follows a strict set of rules to determine compatibility:\n",
        "\n",
        ". Pad dimensions: If arrays have different dimensions, the smaller array's shape is padded with ones on its leading (left) side to match the number of dimensions of the larger array.\n",
        "\n",
        ". Compare dimensions: NumPy compares the dimensions of the two arrays element-wise, starting from the trailing (rightmost) dimension.\n",
        "\n",
        ". Compatibility: Two dimensions are compatible if they are either equal, or if one of them is 1.\n",
        "\n",
        ". Error: If the sizes in any dimension disagree and neither is 1, a ValueError is raised, and the arrays cannot be broadcast.\n",
        "\n",
        "If the arrays are compatible, the size of the resulting array in each dimension is the maximum of the two input sizes. Dimensions with a size of 1 are \"stretched\" to match the other array's size in that dimension."
      ],
      "metadata": {
        "id": "laq-ACmg1yjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.What is a Pandas DataFrame\n",
        "Ans. A Pandas DataFrame is a two-dimensional, mutable, tabular data structure with labeled axes (rows and columns), similar to a spreadsheet or SQL table. It is the primary object in the Pandas library for Python data analysis and manipulation.\n",
        "\n",
        ".Structure: Data is organized in rows and columns, allowing for a mix of different data types within different columns.\n",
        "\n",
        ". Indexing: Both rows and columns have labels (indices) which allow for flexible and efficient access and manipulation of data using label-based (.loc[]) or position-based (.iloc[]) indexing.\n",
        "\n",
        ". Functionality: It provides powerful methods for data cleaning, transformation, aggregation (like groupby), filtering, sorting, and handling missing data (represented as NaN).\n",
        "\n",
        ". Compatibility: DataFrames can be created from various data sources, including CSV files, Excel spreadsheets, SQL databases, Python dictionaries, and NumPy arrays, and they integrate well with other Python scientific computing libraries like Matplotlib and Scikit-learn.\n"
      ],
      "metadata": {
        "id": "lk3L4ENZ2XCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Explain the use of the groupby() method in Pandas\n",
        "ANS.The pandas.groupby() method is used to split a DataFrame or Series into groups based on one or more criteria, and then apply a function (aggregation, transformation, or filtration) to each group independently. The process is known as the \"split-apply-combine\" strategy.\n",
        "\n",
        "Split: The data is divided into groups based on unique values in the specified column(s). This step is lazy; it creates a GroupBy object that holds grouping information but doesn't perform calculations immediately.\n",
        "\n",
        "Apply: A function is applied to each individual group. This can be an aggregation (e.g., sum(), mean(), count()), a transformation (e.g., rank(), fillna()), or a filtration (e.g., filter()).\n",
        "\n",
        "Combine: The results of the function applications are combined into a new Series or DataFrame. By default, the group names become the index of the new structure.\n",
        "\n",
        "This allows for efficient summarization and analysis of large datasets across different categories, similar to the GROUP BY clause in SQL. You can find more details in the official pandas documentation.\n"
      ],
      "metadata": {
        "id": "lysHNZ7123Ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Why is Seaborn preferred for statistical visualizations\n",
        "ans .Seaborn is preferred for statistical visualization because it simplifies complex plots, integrates seamlessly with Pandas DataFrames, offers beautiful default themes/colors, and automatically provides statistical insights like confidence intervals, making exploratory data analysis (EDA) faster and more insightful with less code, allowing focus on data meaning rather than drawing details.\n",
        "\n",
        "Key Reasons for Preference:\n",
        "\n",
        ". High-Level Interface: It abstracts away much of Matplotlib's complexity, letting users create sophisticated plots (pair plots, violin plots, heatmaps) with fewer lines of code.\n",
        "\n",
        ". Pandas Integration: Works directly with DataFrames, making plotting from structured data intuitive (e.g., sns.plot('col1', 'col2', df)).\n",
        "\n",
        " . Built-in Themes & Palettes: Comes with attractive default themes (like whitegrid, darkgrid) and color palettes, enhancing visual appeal instantly.\n",
        "\n",
        " . Statistical Functionality: Automatically calculates and displays statistical summaries (distributions, regressions, aggregations) with specialized functions.\n",
        "\n",
        " . Data-Centric Approach: Its declarative API focuses on what the plot represents (semantics) rather than how to draw it (details).\n",
        "\n",
        " . Facilitates EDA: Excellent for quickly exploring datasets to uncover patterns and relationships, especially with categorical data.\n",
        "\n",
        "In essence, Seaborn makes creating professional, statistically rich, and aesthetically pleasing plots quick and efficient, especially when dealing with structured data in Python.\n"
      ],
      "metadata": {
        "id": "Lrif3-KF3U5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 .A What are the differences between NumPy arrays and Python lists\n",
        " Ans .NumPy arrays are optimized for fast, memory-efficient numerical operations on large, homogeneous datasets, while Python lists are flexible, general-purpose containers that can store heterogeneous data types.\n",
        "\n",
        ". Data Type: NumPy arrays require all elements to be of the same data type (homogeneous), which enables efficient storage and operations. Python lists can hold elements of different data types (heterogeneous), such as integers, strings, and other objects.\n",
        "\n",
        ". Performance: NumPy operations on large datasets are significantly faster because they are implemented in C and leverage vectorized operations, avoiding explicit Python loops. Python lists are slower for numerical tasks, especially with large amounts of data, due to per-element type checking overhead.\n",
        "\n",
        ". Memory Efficiency: NumPy arrays are more memory-efficient as they store data in contiguous memory blocks, reducing overhead. Python lists store pointers to objects scattered in memory, which consumes more memory per element.\n",
        "\n",
        ". Size: NumPy arrays have a fixed size upon creation, while Python lists are dynamic and can grow or shrink in size as needed.\n",
        "\n",
        " . Functionality: NumPy provides an extensive collection of optimized mathematical functions (e.g., linear algebra, statistics) and supports advanced features like broadcasting and multi-dimensional arrays. Python lists offer many general-purpose built-in methods like append(), insert(), and sort().\n"
      ],
      "metadata": {
        "id": "gO26PD8e4Xpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.A What is a heatmap, and when should it be used\n",
        "Ans . A heatmap is a data visualization that uses color intensity to represent values in a matrix, showing patterns, correlations, or user behavior at a glance, ideal for spotting trends, high-engagement areas (like clicks/scrolls on a webpage), or anomalies in large datasets, used by UX/marketing (web optimization), finance, and science (geography, biology) to quickly grasp complex info.\n",
        "\n",
        "What is a Heatmap?\n",
        "\n",
        ".Color-Coded Data: It's a grid or map where each cell's color signifies a data value, with warmer colors (reds/oranges) for high values (\"hot\") and cooler colors (blues/greens) for low values (\"cold\").\n",
        "\n",
        ". Visual Encoding: Instead of numbers in a table, colors communicate magnitude, making complex data instantly understandable.\n",
        "\n",
        "Types:\n",
        "\n",
        ". Web Analytics: Shows clicks, mouse movements (hover maps), and scroll depth on web pages.\n",
        "\n",
        ". Matrix Heatmaps: Visualizes correlations or density in a 2D grid (e.g., daily precipitation, financial data).\n",
        "\n",
        ". Geographic Heatmaps: Displays data density on maps (e.g., population density, disease outbreaks).\n",
        "\n",
        "When to Use a Heatmap:\n",
        "\n",
        ". User Experience (UX) & Marketing: To see where users interact most on a website, optimize layouts, place CTAs, and improve conversion rates.\n",
        "\n",
        ". Product Management: To understand feature usage and user flow within an app.\n",
        "Data Analysis: To find patterns, trends, and outliers in large datasets quickly (e.g., financial data, production defects).\n",
        "\n",
        "Scientific & Geographical Research: To visualize distributions, such as climate patterns or population data.\n",
        "\n",
        ". Sports Analytics: To analyze player movement, strategy, or performance patterns.\n",
        "\n",
        ". Cybersecurity: To spot unusual access patterns or malicious activity in network logs.\n",
        "\n",
        "Key Benefits:\n",
        "\n",
        ". Quick Insights: Identifies patterns and high/low-engagement areas at a glance.\n",
        "\n",
        ". Reduced Cognitive Load: Processes visual information faster than text/numbers.\n",
        "\n",
        ". Prioritization: Helps decide where to focus optimization efforts.\n"
      ],
      "metadata": {
        "id": "hbAWUYsh5Hjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.A What does the term “vectorized operation” mean in NumPy\n",
        "Ans .A \"vectorized operation\" in NumPy applies mathematical operations to an entire array at once, rather than iterating through individual elements using Python loops. This leverages optimized, pre-compiled C code for significantly faster performance.\n"
      ],
      "metadata": {
        "id": "FIFqx3-D6SbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. A How does Matplotlib differ from Plotly\n",
        "Ans . Matplotlib is primarily for creating static, highly customizable, publication-quality plots, while Plotly excels at generating interactive, web-based visualizations with modern aesthetics and built-in interactivity like hover effects and zooming.\n",
        "\n",
        "Core Differences\n",
        "\n",
        ". Interactivity: Plotly plots are interactive by default (e.g., hover tooltips, zooming, panning), making them ideal for data exploration and web dashboards. Matplotlib plots are static images by default, requiring additional libraries or complex setups for limited interactivity.\n",
        "\n",
        ". Purpose & Output: Matplotlib was designed for creating static, print-ready graphics for scientific and academic publications. Plotly was built for the web, producing web-ready, dynamic visualizations that can be embedded in web applications.\n",
        "\n",
        ". Aesthetics & Ease of Use: Plotly generally produces more aesthetically pleasing plots out of the box with minimal code. Matplotlib offers unparalleled control over every plot element for granular customization but often requires more verbose code to achieve a polished look.\n",
        "\n",
        ". Ecosystem & Integration: Matplotlib has a vast, long-standing ecosystem that integrates with many scientific Python libraries like NumPy and Pandas. Plotly integrates seamlessly with web frameworks like Dash for building interactive data apps.\n",
        "\n"
      ],
      "metadata": {
        "id": "RcOJd9SW6lFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.A What is the significance of hierarchical indexing in Pandas\n",
        "Ans .Using Hierarchical Indexes With Pandas | by Todd Birchard ...Hierarchical indexing (MultiIndex) in Pandas is significant because it allows handling data with more than two dimensions within 1D Series and 2D DataFrames, enabling sophisticated analysis, efficient grouping, and flexible data slicing by treating related data as atomic units, preserving relationships, and providing powerful tools for organizing and querying complex, multi-level datasets like those with regions, categories, and time periods.\n",
        "\n",
        "Key Significances:\n",
        "\n",
        ". Higher-Dimensional Data: It effectively represents data with three or more dimensions (like Year, Month, Day) in standard Pandas structures (Series, DataFrame).\n",
        "\n",
        ". Sophisticated Queries: Enables complex selections and aggregations, letting you slice and dice data based on multiple index levels (e.g., all data for 'State A' in '2024').\n",
        "\n",
        ". Data Structuring: Helps in organizing data logically, making it easier to manipulate and analyze, similar to nested categories (like Product Category > Subcategory).\n",
        "\n",
        ". Preserves Relationships: Keeps associated data (metadata) intrinsically linked to its rows, preventing misalignment during operations.\n",
        "Efficient Analysis: Facilitates powerful groupby() operations and reshaping (pivot/unstack) for deeper insights, as it treats related index levels as single keys.\n",
        "\n",
        "In essence, MultiIndex turns simple row/column labels into powerful, multi-level keys, unlocking advanced data manipulation and analysis capabilities for complex real-world data.\n"
      ],
      "metadata": {
        "id": "Fc0Awna27ajt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. What is the role of Seaborn’s pairplot() function\n",
        "Ans .Seaborn's pairplot() function creates a matrix of scatterplots to visualize pairwise relationships between all numerical variables in a dataset, offering a comprehensive \"bird's-eye view\" for exploratory data analysis (EDA) to spot trends, correlations, and outliers quickly, with diagonal plots showing univariate distributions (histograms/KDEs) and off-diagonal plots showing bivariate relationships.\n",
        "\n",
        "Key Roles & Features\n",
        "\n",
        ". Automates Pairwise Plots: Generates a grid where each variable gets its own row and column, automatically plotting every combination of variables.\n",
        "\n",
        ". Reveals Relationships: Off-diagonal plots (scatterplots) show how variables interact, helping identify correlations (positive, negative, none).\n",
        "\n",
        ". Shows Distributions: Diagonal plots display the distribution of individual variables (histograms or KDEs), revealing shape and spread.\n",
        "\n",
        ". Facilitates EDA: An essential tool for quickly understanding a dataset's structure, patterns, and potential issues like outliers.\n",
        "\n",
        ". Supports Categorical Data: Can use the hue parameter to color-code points by a categorical variable, revealing group-wise patterns.\n",
        "\n",
        ". Customizable: Allows selection of specific variables for rows/columns (x_vars, y_vars), plot types (kind), and more for tailored views.\n",
        "\n",
        "In essence, pairplot() condenses complex multivariate data into a single, interpretable figure, making it a cornerstone for initial data exploration in Python\n"
      ],
      "metadata": {
        "id": "u-WzcY0c75RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. A What is the purpose of the describe() function in Pandas\n",
        "Ans. The purpose of the pandas describe() function is to generate descriptive statistics that summarize the central tendency, dispersion, and shape of a dataset's distribution, excluding NaN values [1]. It provides quick insights into numerical data, like mean, standard deviation, and quartiles, and provides counts and frequency information for categorical data [1].\n",
        "\n",
        "You can apply the function directly to a DataFrame or a Series:\n",
        "\n",
        "To get statistics for all columns, use df.describe() [1].\n",
        "For specific insights, you can apply it to a single column, e.g., df['column_name'].describe() [1]."
      ],
      "metadata": {
        "id": "K0xQa-Yu8fWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. A Why is handling missing data important in Pandas\n",
        "Ans.Handling missing data is crucial because unprocessed null values lead to incorrect analyses, misleading conclusions, and errors in machine learning models. Proper handling ensures the integrity, accuracy, and reliability of your dataset and results.\n",
        "\n",
        ". Prevents errors: Most data analysis and machine learning libraries cannot process missing values (represented as NaN or None in Pandas), and will return an error.\n",
        "\n",
        ". Ensures accuracy: Leaving missing data unaddressed can bias results, leading to flawed statistical analyses and unreliable models.\n",
        "\n",
        ". Preserves data integrity: Deciding whether to drop or impute missing values (using methods like mean, median, or mode with pandas.DataFrame.fillna or pandas.DataFrame.dropna) helps retain valuable information, rather than arbitrarily discarding entire rows or columns.\n",
        "\n",
        ". Improves model performance: Correctly addressing missing data, often through imputation, generally leads to more robust and accurate predictive models compared to simply ignoring or deleting the data.\n",
        ""
      ],
      "metadata": {
        "id": "PrB65zFS83jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14.What are the benefits of using Plotly for data visualization\n",
        "Ans. Plotly's benefits include creating stunning, highly interactive (zoom, pan, hover) web-based visualizations with minimal code (especially with Plotly Express), supporting a vast range of chart types (2D, 3D, maps), offering deep customization, seamless integration with Python/R/Julia, and enabling easy deployment of complex dashboards and apps for quick data exploration and business insights.\n",
        "\n",
        "Key Advantages:\n",
        "\n",
        "Interactivity: Plots are naturally interactive, allowing users to zoom, pan, filter, and hover for data details without extra code, enhancing exploration.\n",
        "\n",
        ". Ease of Use (Plotly Express): A high-level API that generates complex plots with very few lines of code, ideal for rapid prototyping and data discovery.\n",
        "\n",
        ". Wide Variety of Plots: Supports numerous chart types, including scientific (3D, contour, heatmaps) and geospatial plots, beyond standard bar/line/scatter.\n",
        "\n",
        ". Deep Customization: Full control over colors, fonts, layouts, annotations, and hover templates for publication-quality visuals.\n",
        "\n",
        ". Multi-Language & Platform: Works with Python, R, Julia, and integrates with Jupyter, Dash, and web apps; visuals are browser-friendly.\n",
        "\n",
        ". Seamless Integration: Works well with data tools like pandas, allowing complex data transformations before plotting.\n",
        "\n",
        ". Deployment: Easily export as HTML or build scalable, interactive dashboards with Plotly Dash for web deployment.\n",
        "\n",
        "Real-Time Capabilities: Supports dynamic updates for live data streaming.\n",
        "\n",
        "Best For:\n",
        "\n",
        ". Quickly gaining insights from new datasets.\n",
        "\n",
        ". Creating sophisticated, shareable reports and dashboards.\n",
        "\n",
        ". Interactive scientific and business analysis.\n"
      ],
      "metadata": {
        "id": "PN5Vgtca9T_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. How does NumPy handle multidimensional arrays\n",
        "Ans .NumPy handles multidimensional arrays using a core object called the ndarray (N-dimensional array), which stores homogeneous data in a contiguous block of memory for highly efficient operations. It provides features like vectorized operations, versatile indexing, and broadcasting to manage data across any number of dimensions (axes).\n",
        "\n",
        ". Efficient Storage: All elements in an ndarray must be of the same data type (e.g., all integers or all floats), which allows NumPy to store them in a single, contiguous memory block. This design eliminates the memory overhead of Python lists and enables faster data retrieval and processing.\n",
        "\n",
        ". Vectorized Operations: NumPy applies mathematical and logical operations to an entire array at once, rather than requiring explicit Python loops, a process called vectorization. These operations are implemented in optimized C code, significantly speeding up computations.\n",
        "\n",
        ". Indexing and Slicing: Elements and subarrays are accessed using a comma-separated tuple of indices for each dimension, such as arr[i, j, k]. Slicing operations (e.g., arr[:, 1:3]) return a view of the original data rather than a copy, which further saves memory.\n",
        "\n",
        ". Broadcasting: This powerful mechanism allows arithmetic operations on arrays of different, but compatible, shapes and dimensions. The smaller array is conceptually \"stretched\" to match the shape of the larger one without actually creating copies in memory, making code concise and efficient.\n",
        "\n",
        ". Shape Manipulation: NumPy provides functions like reshape(), flatten(), and transpose() to change the dimensions and layout of an array without necessarily moving the underlying data in memory. The .shape attribute provides a tuple representing the size of the array along each axis.\n"
      ],
      "metadata": {
        "id": "F23lWxE6-DLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16.A What is the role of Bokeh in data visualization\n",
        "Ans.Start using this Interactive Data Visualization Library ...Bokeh's role in data visualization is to create high-performance, interactive plots for modern web browsers, allowing users to zoom, pan, and hover over data directly in HTML/JavaScript, making it ideal for building dashboards, complex web apps, and exploratory data analysis in Python with beautiful, customizable visuals. It bridges Python code with JavaScript for rich, dynamic outputs, unlike static libraries.\n",
        "\n",
        "Key Roles & Functions:\n",
        "\n",
        ". Interactivity: Enables zooming, panning, hovering for tooltips, and widgets (sliders, dropdowns) directly in the browser for deep data exploration.\n",
        "\n",
        ". Web-Ready Output: Renders visualizations as HTML, JavaScript, or embeds them in Flask/Django apps, perfect for sharing online.\n",
        "\n",
        ". High-Level & Low-Level APIs: Offers bokeh.plotting for quick plots and bokeh.models for fine-grained control over complex applications.\n",
        "\n",
        ". Complex Visualizations: Supports diverse plots (lines, bars, maps, etc.) and layouts (grids, tabs) for sophisticated dashboards.\n",
        "\n",
        ". Bokeh Server: Allows for server-backed apps, handling large/streaming data, and creating complex interactions with Python callbacks.\n",
        "\n",
        "In essence:\n",
        "\n",
        "Bokeh lets Python developers build stunning, interactive data applications that run on the web, transforming static charts into dynamic, explorable tools for data storytelling and analysis.\n"
      ],
      "metadata": {
        "id": "WQpVOGxx-j8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Explain the difference between apply() and map() in Pandas\n",
        "Ans. map() is strictly for Series and performs element-wise value substitution, while apply() works on both Series and DataFrames to apply functions along an axis (rows or columns).\n",
        "\n",
        "map() in Pandas\n",
        "\n",
        ". Scope: Works exclusively on a Series (a single column in a DataFrame).\n",
        "\n",
        ". Operation: Used for element-wise transformation or substitution of values.\n",
        "\n",
        ". Input: Accepts a function, a dictionary, or another Series to define the mapping logic. When using a dictionary or Series, it is highly optimized for performance.\n",
        "\n",
        ". Use Case: Ideal for tasks like replacing categorical string values with numerical codes or mapping values based on a lookup table.\n",
        "\n",
        "apply() in Pandas\n",
        "\n",
        ". Scope: Can be used on both Series and DataFrames.\n",
        "\n",
        ". Operation: Applies a function to an entire row or column at once (or element-wise for functions that support broadcasting).\n",
        "\n",
        ". Input: Primarily accepts a Python function (callable). It also allows passing additional positional or keyword arguments to the function, which map() does not.\n",
        "\n",
        ". Use Case: Suited for more complex operations, custom aggregations (e.g., calculating a custom statistic for each column), or operations that require logic involving multiple values within a row or column.\n",
        "\n",
        ". In summary: Use map() for simple value lookups or element-wise transformations on a single column and apply() for more complex, multi-element operations across rows or columns. Both are generally less efficient than built-in vectorized pandas or NumPy functions, so prefer those when possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "VcVjaeE-_ND1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. What are some advanced features of NumPy\n",
        "Ans. Advanced features of NumPy significantly enhance performance and enable complex data manipulation through efficient operations that avoid slow Python loops.\n",
        "\n",
        "Key advanced features include:\n",
        "\n",
        ". Vectorization and Universal Functions (ufuncs): Performing high-speed, element-wise operations on entire arrays using optimized C functions, eliminating the need for explicit Python loops.\n",
        "\n",
        ". Broadcasting: Automatically handling operations on arrays with different shapes or dimensions by conceptually expanding the smaller array to match the larger one without data duplication, saving memory and processing time.\n",
        "\n",
        ". Fancy Indexing and Boolean Masking: Selecting and modifying complex subsets of data using arrays of indices or boolean masks, which is more powerful and concise than basic slicing.\n",
        "\n",
        ". Structured Arrays: Working with heterogeneous data where each element can have multiple fields with different data types, similar to a record or a database row.\n",
        "\n",
        ". Linear Algebra Operations: A comprehensive module for high-performance matrix and vector products, finding eigenvalues, determinants, and solving linear equations, which are fundamental in machine learning and scientific computing.\n",
        "\n",
        ". Memory Optimization: Utilizing techniques like memory views (avoiding data copies), choosing appropriate data types, and memory mapping for datasets larger than available RAM.\n",
        "\n",
        ". Signal Processing: Built-in functions for Fourier transforms, filtering, and convolution, crucial for analyzing signals and time-series data.\n"
      ],
      "metadata": {
        "id": "rMTbLfWkAp2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19.A How does Pandas simplify time series analysis\n",
        "Ans. Pandas simplifies time series analysis by providing time-aware data structures (DatetimeIndex, Timestamp, Timedelta) that integrate naturally with DataFrame and Series objects, enabling powerful, intuitive operations not possible with standard Python data types.\n",
        "\n",
        "Key simplifications include:\n",
        "\n",
        ". Easy Conversion The pd.to_datetime() function automatically parses diverse date and time formats into usable Timestamp objects.\n",
        "\n",
        ". Intuitive Indexing and Slicing With a DatetimeIndex, users can filter data by specific years, months, or date ranges using simple string-based commands (e.g., df.loc['2023-01':'2023-03']).\n",
        "\n",
        ". Resampling and Frequency Conversion The .resample() method aggregates or expands data to different time frequencies (e.g., hourly to daily averages, daily to monthly totals), which is crucial for identifying trends.\n",
        "\n",
        ". Rolling and Expanding Windows Functions like .rolling() and .expanding() calculate moving statistics (e.g., moving averages) to smooth data and highlight long-term trends.\n",
        "\n",
        ". Missing Data Handling Pandas provides methods like forward-fill (ffill()) and backward-fill (bfill()) to manage gaps in time series data more effectively than standard imputation methods.\n",
        "\n",
        ". Time Zone Localization and Conversion The library simplifies working with global data by handling time zone conversions and daylight-saving time mechanics automatically.\n"
      ],
      "metadata": {
        "id": "1WukFoYVBM79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. What is the role of a pivot table in Pandas\n",
        "Ans.The role of a Pandas pivot table is to summarize and aggregate large, detailed DataFrames by reshaping data, moving categories from rows to columns (or vice versa) for easier analysis, and performing calculations like sums, averages, or counts to reveal hidden trends and insights. It transforms raw data into a concise, readable format, similar to Excel's pivot tables, making data exploration, pattern identification, and reporting much simpler and more efficient.\n",
        "\n",
        "Key Functions & Benefits:\n",
        "\n",
        ". Data Aggregation: Calculates statistical summaries (mean, sum, count, etc.) for groups of data.\n",
        "\n",
        ". Data Reshaping: Rotates data, turning unique values from one column into new columns, which helps in cross-tabulation.\n",
        "\n",
        ". Insight Discovery: Uncovers patterns, trends, and relationships in data that are hard to see in the raw format.\n",
        "\n",
        ". Flexible Analysis: Allows users to easily change the structure (index, columns, values) to gain different perspectives.\n",
        "\n",
        ". Handling Duplicates: The pivot_table() function can automatically handle duplicate entries by applying an aggregation function (default is mean).\n",
        "\n",
        "Core Arguments in pandas.pivot_table()`:\n",
        "\n",
        ". index: Column(s) to group data by (becomes new rows).\n",
        "\n",
        ". columns: Column(s) whose unique values become new columns.\n",
        "\n",
        ". values: Column(s) to aggregate.\n",
        "\n",
        ". aggfunc: The function to apply (e.g., 'sum', 'mean', 'count', 'min', 'max').\n",
        "margins=True: Adds \"All\" rows/columns for total aggregates.\n",
        "\n",
        "In essence, a Pandas pivot table acts as a powerful tool for data summarization and exploration, turning complex datasets into clear, actionable reports.\n"
      ],
      "metadata": {
        "id": "tijHMP4nBuns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21.Why is NumPy’s array slicing faster than Python’s list slicing\n",
        "Ans.NumPy array slicing is faster than Python list slicing because NumPy arrays store data in a contiguous block of memory and are optimized with C-level functions, enabling efficient, vectorized operations and better CPU cache performance.\n",
        "\n",
        ". Contiguous Memory: NumPy arrays store all elements of the same data type right next to each other in memory, allowing the CPU to access data sequentially and efficiently (locality of reference). Python lists, in contrast, store pointers to objects that can be scattered throughout memory, requiring more reads.\n",
        "\n",
        ". Views vs. Copies: NumPy slicing generally creates a new \"view\" of the data, which is a new array object referencing the same underlying memory, avoiding the time-consuming process of copying large amounts of data. Python list slicing, however, creates a new list with copies of the elements.\n",
        "\n",
        ". Optimized Implementation: NumPy is an extension module written largely in C, meaning its core functions, including slicing operations, run at high speeds without the overhead of Python's interpreter loop for each element.\n"
      ],
      "metadata": {
        "id": "Oe9mZa36CbgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. What are some common use cases for Seaborn?\n",
        "Ans.Seaborn is a Python library primarily used for creating attractive and informative statistical graphics, which is crucial for exploratory data analysis (EDA) and communicating data insights.\n",
        "Common use cases for Seaborn include:\n",
        "\n",
        "Visualizing Statistical Relationships: Seaborn excels at showing relationships between variables, such as how one numerical variable changes as a function of another. This is often done using scatter plots and line plots, with built-in functionality to automatically perform statistical estimation and display confidence intervals.\n",
        "\n",
        ".Understanding Data Distributions: It is widely used to visualize the distribution, spread, and shape of variables in a dataset. Common plots for this purpose include histograms, kernel density estimation (KDE) plots, and rug plots.\n",
        "\n",
        ". Analyzing Categorical Data: Seaborn provides specialized plot types for comparing data across different categories. This includes bar plots (for comparing averages), count plots (for frequency analysis), and box/violin plots (for visualizing distributions within categories).\n",
        "\n",
        ". Exploring Multivariate Datasets: It simplifies the visualization of complex datasets with multiple variables. For instance, the pairplot function generates a grid of pairwise relationships across an entire DataFrame, and heatmaps are used to visualize correlation matrices.\n",
        "\n",
        ". Automating Aesthetics and Styling: Seaborn offers high-level, dataset-oriented APIs and built-in themes that create professional-looking plots with minimal code, saving time on manual styling (a common task with its underlying library, Matplotlib).\n",
        "\n",
        ". Time Series Analysis: It is well-suited for plotting time-based data to identify trends, seasonality, and other patterns over time using functions like lineplot().\n",
        "\n",
        ". Regression Analysis: Specialized functions like lmplot() and regplot() are used to visualize linear relationships between variables, including fitting and displaying regression lines along with their uncertainty bands.\n",
        "\n",
        "In essence, Seaborn is a go-to tool for data scientists and analysts who need to quickly explore, understand, and present patterns and trends in their data using visually appealing statistical graphics.\n"
      ],
      "metadata": {
        "id": "Xfw0V3DcDD_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "_RzBXRp3Dr-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.How do you create a 2D NumPy array and calculate the sum of each row\n",
        "Ans.To create a 2D NumPy array, use numpy.array() with a list of lists. Calculate the sum of each row using the .sum() method with axis=1.\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "    # Create the 2D array\n",
        "    data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "    array_2d = np.array(data)\n",
        "\n",
        "    # Calculate the sum of each row (axis=1)\n",
        "    row_sums = array_2d.sum(axis=1)\n",
        "\n",
        "    print(\"2D Array:\")\n",
        "    print(array_2d)\n",
        "    print(\"\\nRow Sums:\")\n",
        "    print(row_sums)\n"
      ],
      "metadata": {
        "id": "KhT4asJWEEya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Write a Pandas script to find the mean of a specific column in a DataFrame\n",
        "Ans.To find the mean of a specific column, use the dot notation or bracket syntax to select the column as a Pandas Series and then apply the .mean() method directly to it.\n",
        "\n",
        "python\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "    # Example DataFrame setup\n",
        "    data = {'A': [10, 20, 30, 40],\n",
        "        'B': [15, 25, 35, 45]}\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Calculate the mean of column 'A' using dot notation\n",
        "    mean_a = df.A.mean()\n",
        "\n",
        "    # Calculate the mean of column 'B' using bracket syntax\n",
        "    mean_b = df['B'].mean()\n",
        "\n",
        "    print(f\"Mean of column A: {mean_a}\")\n",
        "    print(f\"Mean of column B: {mean_b}\")\n"
      ],
      "metadata": {
        "id": "9ga_nTJTFPC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Create a scatter plot using Matplotlib\n",
        "Ans.To create a scatter plot in Matplotlib, you use the plt.scatter() function, providing data for the X and Y axes.\n",
        "\n",
        "python\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "    # Sample data\n",
        "    x = np.array([5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6])\n",
        "    y = np.array([99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86])\n",
        "\n",
        "    # Create the scatter plot\n",
        "    plt.scatter(x, y)\n",
        "\n",
        "    # Add labels and a title\n",
        "    plt.xlabel(\"X-axis Label\")\n",
        "    plt.ylabel(\"Y-axis Label\")\n",
        "    plt.title(\"My Scatter Plot\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        " . First, import the necessary libraries: matplotlib.pyplot (conventionally as plt) and numpy (as np).\n",
        "\n",
        " . Define your data as lists or arrays for your x and y coordinates.\n",
        "\n",
        ". Call plt.scatter(x, y) with your data.\n",
        "\n",
        ". Use plt.xlabel(), plt.ylabel(), and plt.title() to add context.\n",
        "\n",
        ". Finally, call plt.show() to display the visualization.\n",
        "\n",
        "Optional parameters like c (color), s (size), marker (shape), and alpha (transparency) can be used within plt.scatter() for enhanced customization.\n"
      ],
      "metadata": {
        "id": "JJ76LaTmFrNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.How do you calculate the correlation matrix using Seaborn and visualize it with a heatmap\n",
        "Ans.To calculate and visualize a correlation matrix with Seaborn, first use pandas' .corr() on your DataFrame to get the matrix, then pass it to sns.heatmap() with annot=True for values, a cmap (like 'coolwarm') for colors, and vmin/vmax (-1, 1) for range, ensuring your numerical data is ready for analysis.\n",
        "Step-by-Step Guide\n",
        "\n",
        "python\n",
        "\n",
        "# 1. Import Libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Often needed for masks or sample data\n",
        "\n",
        "# 2. Load Your Data (Example using built-in dataset)\n",
        "# df = pd.read_csv('your_data.csv') # For your data\n",
        "df = sns.load_dataset('iris') # Using Iris dataset for example\n",
        "\n",
        "# 3. Calculate the Correlation Matrix\n",
        "# .corr() automatically selects numeric columns and computes pairwise correlations\n",
        "correlation_matrix = df.corr(numeric_only=True)\n",
        "\n",
        "# 4. Visualize with Seaborn Heatmap\n",
        "plt.figure(figsize=(10, 8)) # Adjust figure size as needed\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    annot=True,       # Show correlation values on the map\n",
        "    cmap='coolwarm',  # Color map (reds for positive, blues for negative)\n",
        "    fmt=\".2f\",        # Format annotations to 2 decimal places\n",
        "    linewidths=.5,    # Adds lines between cells\n",
        "    vmin=-1,          # Set min value for color scale\n",
        "    vmax=1            # Set max value for color scale\n",
        ")\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()\n",
        "This video provides a quick demonstration of creating a correlation heatmap in Python:\n",
        "Related video thumbnail\n",
        "1m\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Key sns.heatmap() Parameters\n",
        "\n",
        "data: The correlation matrix (from df.corr()).\n",
        "\n",
        "annot=True: Displays the correlation coefficients in each cell.\n",
        "\n",
        "cmap='coolwarm': Sets the color palette; 'RdBu' (Red-Blue) or 'viridis' are also popular.\n",
        "\n",
        "fmt=\".2f\": Formats the annotation text (e.g., to two decimal places).\n",
        "vmin=-1, vmax=1: Ensures the color scale spans the full -1 to +1 range, crucial for correlation plots.\n",
        "\n",
        "square=True: Makes cells square for better visual symmetry.\n",
        "mask: Use np.triu(correlation_matrix) to hide the upper triangle for cleaner plots.\n"
      ],
      "metadata": {
        "id": "6tiYxnSgGTCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Generate a bar plot using Plotly\n",
        "Ans. To generate an interactive bar plot in Python using Plotly, you can use the high-level plotly.express API with just a few lines of code.\n",
        "\n",
        "Steps and Example Code\n",
        "\n",
        "First, ensure you have Plotly installed (pip install plotly pandas). Then, you can use the following Python code:\n",
        "\n",
        "python\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Prepare sample data (can also load from a file)\n",
        "data = {\n",
        "    'Category': ['A', 'B', 'C', 'D'],\n",
        "    'Value': [15, 22, 18, 28]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Create the bar plot using plotly express\n",
        "fig = px.bar(df, x='Category', y='Value', title='Sample Plotly Bar Chart', text_auto=True)\n",
        "\n",
        "# 3. Customize and display the plot\n",
        "fig.update_layout(\n",
        "    xaxis_title='Categories',\n",
        "    yaxis_title='Measured Value',\n",
        "    title_x=0.5\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "Key Components\n",
        "\n",
        ". import plotly.express as px: Imports the Plotly Express module, which is the easiest way to create graphs.\n",
        "\n",
        ". px.bar(): This function creates the bar chart. You specify the DataFrame, the columns for the x and y axes, and optional parameters like title and text_auto to automatically show values on the bars.\n",
        "\n",
        ". fig.show(): This method displays the interactive figure in your environment (browser, Jupyter notebook, etc.).\n",
        "\n",
        "For more advanced customization, you can use the lower-level plotly.graph_objects API, though it requires more code. The Plotly documentation provides extensive examples and styling options.\n"
      ],
      "metadata": {
        "id": "dqCqdItrHGOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Create a DataFrame and add a new column based on an existing column\n",
        "Ans.You can create a pandas DataFrame using a Python dictionary and add a new column via direct assignment or vectorized operations on existing columns.\n",
        "\n",
        "python\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "    # 1. Create a DataFrame from a dictionary\n",
        "    data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Original DataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "    # 2. Add a new column 'Age in 5 Years' based on the 'Age' column\n",
        "    df['Age in 5 Years'] = df['Age'] + 5\n",
        "    print(\"\\nDataFrame with new column:\")\n",
        "    print(df)\n",
        "Alternative Methods for Complex Logic\n",
        "numpy.where() For simple if-else conditions, numpy.where() is highly efficient.\n",
        "\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "    df['is_adult'] = np.where(df['Age'] >= 18, 'Yes', 'No')\n",
        "    # print(df)\n",
        "    df.apply() For more complex functions applied row-by-row, use apply() with axis=1.\n",
        "    python\n",
        "    def age_category(age):\n",
        "    if age < 30:\n",
        "        return 'Young'\n",
        "    else:\n",
        "        return 'Old'\n",
        "\n",
        "    df['Category'] = df['Age'].apply(age_category)\n",
        "    # print(df)\n"
      ],
      "metadata": {
        "id": "6WrP7Fv1H80x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Write a program to perform element-wise multiplication of two NumPy arrays\n",
        "Ans.To perform element-wise multiplication of two NumPy arrays, you can use the multiplication operator (*) or the numpy.multiply() function as shown in this Python program:\n",
        "\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "    # Define two example numpy arrays\n",
        "    array1 = np.array([1, 2, 3, 4])\n",
        "    array2 = np.array([5, 6, 7, 8])\n",
        "\n",
        "    # Method 1: Using the multiplication operator (*)\n",
        "    result_operator = array1 * array2\n",
        "    print(f\"Result using operator: {result_operator}\")\n",
        "\n",
        "    # Method 2: Using the numpy.multiply() function\n",
        "    result_function = np.multiply(array1, array2)\n",
        "    print(f\"Result using function: {result_function}\")\n"
      ],
      "metadata": {
        "id": "5frOqroMIoQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.Create a line plot with multiple lines using Matplotlib\n",
        "Ans.You can create a line plot with multiple lines in Matplotlib by calling the plt.plot() function multiple times before displaying the plot. This approach automatically assigns different colors to each line, making the plot easy to read.\n",
        "\n",
        "python\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "    # Sample data for multiple lines\n",
        "    x_data = [0, 1, 2, 3, 4, 5]\n",
        "    y1_data = [1000, 13000, 26000, 42000, 60000, 81000]\n",
        "    y2_data = [1000, 13000, 27000, 43000, 63000, 85000]\n",
        "\n",
        "    # Plot each line using separate plot() calls\n",
        "    plt.plot(x_data, y1_data, label='Line 1 Data')\n",
        "    plt.plot(x_data, y2_data, label='Line 2 Data', linestyle='--') # Use different line styles if needed\n",
        "\n",
        "    # Add labels, title, and a legend\n",
        "    plt.xlabel(\"X-axis Label\")\n",
        "    plt.ylabel(\"Y-axis Label\")\n",
        "    plt.title(\"Multiple Lines in a Single Plot\")\n",
        "    plt.legend() # Displays the labels defined in each plt.plot() call\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "    Import the library: import matplotlib.pyplot as plt is necessary to access plotting functions.\n",
        "    Call plt.plot() repeatedly: Each call adds a new line to the same figure.\n",
        "  Add a legend: Use plt.legend() to identify which line corresponds to which data set.\n",
        "  \n",
        "Display the figure: plt.show() renders the final graph with all lines visible.\n"
      ],
      "metadata": {
        "id": "WYRYwIi5JK4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.Generate a Pandas DataFrame and filter rows where a column value is greater than a threshold\n",
        "Ans.To generate a pandas DataFrame and filter rows, first, import pandas and create a DataFrame using a Python dictionary or list structure. Then, apply boolean indexing by selecting rows where the specified column meets the greater-than condition.\n",
        "\n",
        "python\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "    # 1. Generate a Pandas DataFrame\n",
        "    data = {'A': [10, 20, 30, 40, 50],\n",
        "        'B': [1, 2, 3, 4, 5]}\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 2. Filter rows where column 'A' value is greater than a threshold (e.g., 25)\n",
        "    threshold = 25\n",
        "    filtered_df = df[df['A'] > threshold]\n",
        "\n",
        "    # Print the original and filtered DataFrames (optional, for demonstration)\n",
        "    # print(\"Original DataFrame:\")\n",
        "    # print(df)\n",
        "    # print(\"\\nFiltered DataFrame (A > 25):\")\n",
        "    # print(filtered_df)\n",
        "The resulting filtered_df will contain only the rows where the values in column 'A' are greater than 25.\n"
      ],
      "metadata": {
        "id": "bshAVKHOJtJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.Create a histogram using Seaborn to visualize a distribution\n",
        "Ans.To create a Seaborn histogram, use the sns.histplot() function, specifying the dataset and the variable for the x-axis. This code loads sample data and plots a basic histogram:\n",
        "\n",
        "python\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "    # Load a sample dataset (e.g., \"penguins\")\n",
        "    penguins = sns.load_dataset('penguins')\n",
        "\n",
        "    # Create the histogram\n",
        "    sns.histplot(data=penguins, x='body_mass_g')\n",
        "\n",
        "    # Optional: Add title and labels using Matplotlib\n",
        "    plt.title('Penguin Body Mass Distribution')\n",
        "    plt.xlabel('Body Mass (g)')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "    Customization Options\n",
        "\n",
        ". Adjust bins: Use the bins parameter to control the number of intervals, e.g., sns.histplot(..., bins=30).\n",
        "\n",
        ". Add a KDE curve: Set kde=True to overlay a Kernel Density Estimation line, which smooths the distribution visualization, e.g., sns.histplot(..., kde=True).\n",
        "\n",
        ". Color by category: Use the hue parameter to color histogram bars based on a categorical variable (e.g., by species), e.g., sns.histplot(..., hue='species').\n",
        "\n",
        ".Change display stat: Modify the stat parameter to show density, probability, or percent instead of the default count.\n",
        "\n"
      ],
      "metadata": {
        "id": "-sZySh8dKKOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.Perform matrix multiplication using NumPy\n",
        "Ans.To perform matrix multiplication in NumPy, use np.matmul(), the @ operator, or np.dot() for the standard matrix product (rows of first * columns of second), or np.multiply() or * for element-wise multiplication (Hadamard product), with matmul() or @ being preferred for modern linear algebra.\n",
        "\n",
        "Here's how to do it with examples:\n",
        "\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "    # Define two matrices (NumPy arrays)\n",
        "    matrix_A = np.array([[1, 2], [3, 4]])\n",
        "    matrix_B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "    # 1. Using np.matmul() (Recommended for matrix product)\n",
        "    matrix_product_matmul = np.matmul(matrix_A, matrix_B)\n",
        "    print(\"Using np.matmul():\\n\", matrix_product_matmul)\n",
        "    # Output: [[19 22], [43 50]]\n",
        "\n",
        "    # 2. Using the @ operator (Pythonic way for matrix product)\n",
        "    matrix_product_at = matrix_A @ matrix_B\n",
        "    print(\"\\nUsing @ operator:\\n\", matrix_product_at)\n",
        "    # Output: [[19 22], [43 50]]\n",
        "\n",
        "    #  3. Using np.dot() (Also works for 2D matrices, but matmul preferred)\n",
        "    matrix_product_dot = np.dot(matrix_A, matrix_B)\n",
        "    print(\"\\nUsing np.dot():\\n\", matrix_product_dot)\n",
        "    # Output: [[19 22], [43 50]]\n",
        "\n",
        "    # 4. For Element-wise Multiplication (Hadamard Product)\n",
        "    elementwise_product = np.multiply(matrix_A, matrix_B)\n",
        "    # Or simply: elementwise_product = matrix_A * matrix_B\n",
        "    print(\"\\nElement-wise (np.multiply):\\n\", elementwise_product)\n",
        "    # Output: [[ 5 12], [21 32]]\n",
        "\n",
        "Key Differences:\n",
        "\n",
        ". np.matmul() / @ / np.dot() (for 2D): Performs standard linear algebra matrix multiplication (rows * columns).\n",
        "\n",
        ". np.multiply() / *: Multiplies corresponding elements.\n",
        "\n",
        "For true matrix multiplication, use np.matmul() or the @ operator for clarity and consistency with linear algebra.\n"
      ],
      "metadata": {
        "id": "5KlH2hJ7LQ3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.Use Pandas to load a CSV file and display its first 5 rows\n",
        "Ans.To load a CSV file using pandas and display its first 5 rows, use the following Python code:\n",
        "\n",
        "python\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv('your_file_name.csv')\n",
        "\n",
        "    # Display the first 5 rows\n",
        "    print(df.head())\n",
        "    \n",
        "Replace 'your_file_name.csv' with the actual path to your CSV file. For more details on these functions, refer to the pandas API documentation.\n"
      ],
      "metadata": {
        "id": "Hejnma3DMFdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.Create a 3D scatter plot using Plotly.\n",
        "Ans.A 3D scatter plot can be created using the plotly.express library in Python with the px.scatter_3d() function. This function allows you to map data columns to the x, y, and z axes, and optionally use color, size, and symbols to represent additional dimensions.\n",
        "\n",
        "Python Example Code\n",
        "\n",
        "This example uses the built-in Iris dataset to create an interactive 3D scatter plot.\n",
        "\n",
        "python\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "    # Load the built-in Iris dataset\n",
        "    df = px.data.iris()\n",
        "\n",
        "    # Create the 3D scatter plot\n",
        "    # x, y, and z define the axes\n",
        "    # color defines the color of points based on a variable\n",
        "    fig = px.scatter_3d(df, x='sepal_length', y='sepal_width', z='petal_width',\n",
        "                    color='species', hover_data=['petal_length'])\n",
        "\n",
        "    # Display the plot\n",
        "    fig.show()\n",
        "    Key Features and Customization\n",
        "\n",
        ". Interactivity: The resulting plot is interactive, allowing you to rotate the 3D space by clicking and dragging, zoom in/out with the mouse wheel, and hover over points to see exact values.\n",
        "\n",
        ". Customization: You can customize the plot using various parameters in px.scatter_3d() or by updating the figure's layout and traces.\n",
        "\n",
        ". Labels: Use the labels argument to provide clear axis titles (e.g., labels={'sepal_length': 'Sepal Length (cm)'}).\n",
        "\n",
        ". Size: Map a data column to the size parameter to vary the point size.\n",
        "Opacity: Adjust the opacity of the points to better visualize overlapping data in dense plots.\n",
        "\n",
        ". Alternative Method: For more control, you can use the lower-level go.Scatter3d class from plotly.graph_objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "zHyf5NzzMYNg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zaO4AS0g0ilD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fBvQRK-VNMvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zbHt7HQu0wXD"
      }
    }
  ]
}